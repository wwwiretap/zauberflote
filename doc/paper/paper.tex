\documentclass[letterpaper,twocolumn,10pt]{article}

\usepackage{style, verbatim, url, graphicx}
\usepackage[colorlinks=true, citecolor=blue, linkcolor=blue]{hyperref}
\usepackage{caption}
\usepackage{cleveref, breakurl}

\makeatletter
\newcommand{\crefnames}[3]{
    \@for\next:=#1\do{
        \expandafter\crefname\expandafter{\next}{#2}{#3}
    }
}
\makeatother
\crefnames{part,chapter,section}{\S}{\S\S}

\newcommand{\zbf}{Zauberfl\"{o}te}
\newcommand{\projtitle}{\zbf: A Transparent Peer-to-Peer CDN}
\newcommand{\inclfigure}[3]{
    \begin{figure}
        \begin{centering}
            \includegraphics[width=.8\linewidth]{#2}
            \caption{#3}
            \label{#1}
        \end{centering}
    \end{figure}
}

\begin{document}

\title{\Large \bf \projtitle}

\author{
    {\rm Anish Athalye}\\
    aathalye@mit.edu
    \and
    {\rm Ankush Gupta}\\
    ankush@mit.edu
    \and
    {\rm Katie Siegel}\\
    ksiegel@mit.edu
}

\maketitle
\thispagestyle{empty}

\begin{abstract}

\end{abstract}

\section{Introduction}

% TODO Ankush

\section{\zbf\ API}

% TODO Ankush

\section{Implementation}

The \zbf\ architecture consists of several loosely-connected components (see
figure~\ref{fig:components}). At the highest level, there is a download manager that
provides a reliable download service. It relies on the tracker interface and
the connection manager. The tracker interface facilitates communication with
the central tracker for information about peers and file sizes, and the
connection manager provides an interface for peer-to-peer message transfer.

Our components rely on two standard APIs provided by modern browsers,
WebSockets~\cite{w3c:websocket} for client-server communication and
WebRTC~\cite{w3c:webrtc} for peer-to-peer communication.

\inclfigure{fig:components}{components.pdf}{
    Relationship between different client-side components of the \zbf\
    peer-to-peer download service. Components in gray are standard browser
    APIs.
}

\subsection{Connection manager}

The connection manager is responsible for establishing connections between
peers and facilitating peer-to-peer message transfers. It is a thin layer on
top of WebRTC that handles maintaining connections with multiple peers,
associating peers with peer IDs that are assigned by our tracker. It manages
all signaling through the signaling channel, which implements signaling through
our central server using WebSockets. At this time, this signaling server is the
same server that is running the tracker, but they are logically separate
components, so theoretically, they could be decoupled if needed for better load
balancing.

\subsection{Download manager}

The download manager is the top-level client interface for peer-to-peer
downloads. It provides an API where the client can provide a hash, and
optionally, a fallback HTTP URL, and the download manager will asynchronously
download the content associated with the hash and call a user-provided callback
function when finished. The download manager implements reliable download,
preferring peer-to-peer over HTTP when possible.

\subsubsection{Chunking}

\zbf\ divides assets into chunks; chunks are individually requested and
sent between peers. We use a standard chunk size of 10240 bytes in our
implementation; preliminary tests showed this chunk size to be reasonable.
When a client asks for a resource, the download manager for that client
determines the number of chunks for that resource from the specified size.
The download manager then evenly distributes requests for these chunks
among the peers with the resource. \\
Requests for chunks time out on a per-chunk basis. We track the time \zbf\
last sent a request for each chunk; if this time exceeds a cutoff, we randomly
choose another peer with the resource and send the chunk request to that
peer. We periodically retrieve an updated peers list for this resource
from the tracker.

\subsubsection{Hash validation}

When fetching content peer-to-peer, it is necessary to authenticate data that
is downloaded. \zbf\ achieves this by using a cryptographic hash function to
authenticate data that is downloaded. Assets that are to be downloaded
peer-to-peer are identified by their hash, and once the download manager
finishes downloading the content from peers, it verifies that the hash of the
contents matches what is expected. The download manager rejects content if
there is an authentication error. In the current implementation, if this
occurs, the download manager falls back to downloading the content over HTTP
from the fallback URL.

\subsubsection{Parallelization}

\zbf\ divides assets into chunks primarily to enable parallelization. Because
individual peers may not have high outbound bandwidth, it is beneficial to
download in parallel from many peers. Our implementation parallelizes chunk
downloads, with pending chunk requests balanced between available peers in the
network. If certain peers are slow to respond, the download manager
automatically requests chunks from faster peers to optimize download time.
Parallel downloads help reduce download latency.

\subsubsection{Fault tolerance}

Our download manager implements reliable download semantics. The implementation
is resilient to peer failure, automatically re-requesting chunks from live
peers when necessary. In the worst case, even if all peers go down, the
implementation falls back to downloading over pure HTTP from the centralized
server as a last resort. In any case, the download manager guarantees that the
data will be downloaded and that the download will be authenticated.

\subsection{Tracker interface}

Via the tracker interface, the download manager can request a list of peers that
have a certain resource, specifying the desired resource via that resource's
unique SHA1 hash. After a client receives the full resource, the download
manager publicizes that this client has the resource via this tracker interface,
letting the central tracker know that it possesses the resource.

\subsection{Signaling channel}

The signaling channel enables
% katie

\subsection{DOM injection layer}

% Ankush

\section{Server implementation}

There are two main server-side components to \zbf\. First is the tracker,
which maintains a mapping from resource hashes to the peers that have that
resource. Second is the WebRTC handshake channel, which passes information
between peers to enable them to open WebRTC data channels.

\subsection{Tracker}
The tracker is hosted on a remote server and tracks client connections.
The client-side tracker interface  opens a WebSocket connection with the remote tracker,
which registers the existence of this client. The remote tracker responds to
requests from the client for lists of peers with a given resource. When a client
publicizes that it has a resource through the tracker interface, the remote tracker updates
the list of peers with the resource to include this client, as
long as its WebSocket connection remains open. Upon WebSocket disconnection,
the tracker ``forgets'' about the client and erases all data about the client's
held resources.

\subsection{WebRTC handshake channel}

WebRTC handshake information--the offers, ICE candidates, and answers--are
sent through the client-side signaling channel through a websocket to
the server. The client-side signaling channel specifies a peer to which
this WebRTC handshake information should be sent; if the server has an open
websocket connection with that peer, the server passes that message to the peer.

\section{Evaluation}

% TODO

\section{Future work}

\subsection{Per-chunk hash validation}

% Ankush

\subsection{Hybrid downloads over HTTP and P2P}

% Ankush

\subsection{Variable request sizes}

Implementing variation in requested chunk sizes could allow \zbf\ to independently
determine a more optimal chunk size for each asset. For instance, we hypothesize
that different chunk sizes are optimal for differently sized assets, and potentially
for different media types. If \zbf\ more intelligently determined the optimal size
of chunks for each resource, overall download latency would decrease.

\subsection{Bandwidth detection and prioritization}

If \zbf\ could detect which peers have higher bandwidth connections, and thus
are more capable of swiftly delivering data to a client, \zbf\ could prioritize
requesting data from these peers. Perhaps \zbf\ could request larger chunks from
peers with higher bandwidth connections. \zbf\ could also dynamically react to
poor connections with peers during the chunk requesting process; chunk requests
that subsequently time out could be redistributed more intelligently among
peers with higher bandwidth connections.

\subsection{Support for HTML5 media}

% Ankush


\section{Conclusions}

% Katie


{\footnotesize \bibliographystyle{ieeetr}
\bibliography{paper}}

\end{document}
